<p align="center">
  <h1 align="center">âš¡ ragnarok-ai</h1>
  <p align="center">
    <strong>Local-first RAG evaluation framework for LLM applications</strong>
  </p>
  <p align="center">
    Evaluate, benchmark, and monitor your RAG pipelines â€” 100% locally, no API keys required.
  </p>
</p>

<p align="center">
  <a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/python-3.10+-blue.svg" alt="Python 3.10+"></a>
  <a href="https://www.gnu.org/licenses/agpl-3.0"><img src="https://img.shields.io/badge/License-AGPL--3.0-green.svg" alt="License: AGPL-3.0"></a>
  <a href="https://github.com/astral-sh/ruff"><img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" alt="Ruff"></a>
  <a href="https://mypy-lang.org/"><img src="https://img.shields.io/badge/type%20checked-mypy-blue.svg" alt="Type Checked: mypy"></a>
</p>

<p align="center">
  <a href="#-the-problem">Problem</a> â€¢
  <a href="#-the-solution">Solution</a> â€¢
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-installation">Installation</a> â€¢
  <a href="#-roadmap">Roadmap</a>
</p>

---

## ğŸ”¥ The Problem

Building RAG systems is easy. **Knowing if they actually work is hard.**

Current evaluation tools are either:

| Tool | Issue |
|------|-------|
| **Giskard** | Heavy, slow (45-60 min scans), loses progress on crash, enterprise-focused |
| **RAGAS** | Requires OpenAI API keys, no local-first option |
| **Manual testing** | Doesn't scale, not reproducible |

**You need a tool that:**
- âœ… Runs 100% locally (Ollama, local models)
- âœ… Evaluates fast with checkpointing (no lost progress)
- âœ… Integrates with your existing stack (LangChain, LangGraph)
- âœ… Fits in CI/CD pipelines
- âœ… Doesn't require a PhD to use

---

## ğŸ’¡ The Solution

**ragnarok-ai** is a lightweight, local-first framework to evaluate RAG pipelines.

```python
from ragnarok_ai import evaluate, generate_testset

# Generate test questions from your knowledge base
testset = await generate_testset(
    knowledge_base="./docs/",
    num_questions=50,
    types=["simple", "multi_hop", "adversarial"],
    llm="ollama/mistral",
    checkpoint=True,  # Resume if interrupted
)

# Evaluate your RAG pipeline
results = await evaluate(
    rag_pipeline=my_rag,
    testset=testset,
    metrics=["retrieval", "faithfulness", "relevance"],
    llm="ollama/mistral",
)

# Get actionable insights
results.summary()
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Metric          â”‚ Score â”‚ Status â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Retrieval P@10  â”‚ 0.82  â”‚ âœ…      â”‚
# â”‚ Faithfulness    â”‚ 0.74  â”‚ âš ï¸      â”‚
# â”‚ Relevance       â”‚ 0.89  â”‚ âœ…      â”‚
# â”‚ Hallucination   â”‚ 0.12  â”‚ âœ…      â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

results.export("report.html")
```

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| **ğŸ  100% Local** | Runs entirely on your machine with Ollama. No OpenAI, no API keys, no data leaving your network. |
| **âš¡ Fast & Resilient** | Built-in checkpointing â€” crash mid-evaluation? Resume exactly where you left off. |
| **ğŸ”Œ Framework Agnostic** | Works with LangChain, LangGraph, LlamaIndex, or your custom RAG. |
| **ğŸ“Š Comprehensive Metrics** | Retrieval quality, faithfulness, relevance, hallucination detection, latency tracking. |
| **ğŸ§ª Test Generation** | Auto-generate diverse test sets from your knowledge base. |
| **ğŸš€ CI/CD Ready** | CLI-first design, JSON output, exit codes for pipeline integration. |
| **ğŸª¶ Lightweight** | Minimal dependencies. No torch/transformers in core. |

---

## ğŸ†š Comparison

| Feature | ragnarok-ai | Giskard | RAGAS |
|---------|-------------|---------|-------|
| 100% Local | âœ… | âš ï¸ Partial | âŒ |
| Checkpointing | âœ… | âŒ | âŒ |
| Fast evaluation | âœ… | âŒ (45-60 min) | âœ… |
| CLI support | âœ… | âŒ | âŒ |
| LangChain integration | âœ… | âœ… | âœ… |
| Minimal deps | âœ… | âŒ | âš ï¸ |
| Free & OSS | âœ… AGPL-3.0 | âš ï¸ Open-core | âœ… Apache-2.0 |

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.ai/) running locally
- [uv](https://github.com/astral-sh/uv) (recommended) or pip

### Install

```bash
# Clone
git clone https://github.com/2501Pr0ject/ragnarok-ai.git
cd ragnarok-ai

# Install with uv
uv venv && source .venv/bin/activate
uv pip install -e ".[ollama,qdrant]"
```

### Run your first evaluation

```bash
# CLI (coming soon)
ragnarok evaluate --rag ./my_rag.py --docs ./knowledge_base/ --output report.html

# Or in Python
python examples/basic_evaluation.py
```

---

## ğŸ“¦ Installation

### Using uv (recommended)

```bash
uv venv
source .venv/bin/activate  # Linux/macOS
uv pip install -e "."
```

### Optional dependencies

```bash
# LLM providers
uv pip install -e ".[ollama]"      # Ollama support

# Vector stores  
uv pip install -e ".[qdrant]"      # Qdrant support

# RAG frameworks
uv pip install -e ".[langchain]"   # LangChain/LangGraph support

# Everything
uv pip install -e ".[all]"

# Development
uv pip install -e ".[dev]"
pre-commit install
```

---

## ğŸ¯ Use Cases

### Continuous RAG Testing in CI/CD

```yaml
# .github/workflows/rag-tests.yml
- name: Evaluate RAG Quality
  run: |
    ragnarok evaluate \
      --config ragnarok.yaml \
      --fail-under 0.8 \
      --output results.json
```

### Compare Embedding Models

```python
configs = [
    {"embedder": "nomic-embed-text", "chunk_size": 512},
    {"embedder": "mxbai-embed-large", "chunk_size": 256},
]

results = await benchmark(
    rag_factory=create_rag,
    configs=configs,
    testset=testset,
)
results.compare()  # Side-by-side comparison
```

### Monitor Production Quality

```python
# Track quality drift over time
metrics = await evaluate(rag, production_queries)
metrics.log_to("./metrics/")  # Time-series storage
```

---

## ğŸ“Š Metrics

### Retrieval Metrics
- **Precision@K** â€” Relevant docs in top K results
- **Recall@K** â€” Coverage of relevant docs
- **MRR** â€” Mean Reciprocal Rank
- **NDCG** â€” Normalized Discounted Cumulative Gain

### Generation Metrics
- **Faithfulness** â€” Is the answer grounded in retrieved context?
- **Relevance** â€” Does the answer address the question?
- **Hallucination** â€” Does the answer contain fabricated info?
- **Completeness** â€” Are all aspects of the question covered?

### System Metrics
- **Latency** â€” End-to-end response time
- **Token usage** â€” Cost tracking for LLM calls

---

## ğŸ—ºï¸ Roadmap

### v0.1 â€” Foundation (current)
- [x] Project setup & architecture
- [ ] Core retrieval metrics (precision, recall, MRR)
- [ ] Ollama adapter
- [ ] Qdrant adapter
- [ ] Basic CLI
- [ ] JSON reporter

### v0.2 â€” Generation Metrics
- [ ] Faithfulness evaluator
- [ ] Relevance evaluator
- [ ] Hallucination detection
- [ ] HTML report with visualizations

### v0.3 â€” Test Generation
- [ ] Synthetic question generation
- [ ] Multi-hop question support
- [ ] Adversarial question generation
- [ ] Checkpointing system

### v0.4 â€” Framework Adapters
- [ ] LangChain integration
- [ ] LangGraph integration
- [ ] Custom RAG support

### Future
- [ ] Production monitoring dashboard
- [ ] Rust acceleration for hot paths
- [ ] VS Code extension

---

## ğŸ—ï¸ Project Structure

```
ragnarok-ai/
â”œâ”€â”€ src/ragnarok_ai/
â”‚   â”œâ”€â”€ core/           # Types, protocols, exceptions
â”‚   â”œâ”€â”€ evaluators/     # Metric implementations
â”‚   â”œâ”€â”€ generators/     # Test set generation
â”‚   â”œâ”€â”€ adapters/       # LLM, vector store, framework adapters
â”‚   â”œâ”€â”€ reporters/      # Output formatters (JSON, HTML, console)
â”‚   â””â”€â”€ cli/            # Command-line interface
â”œâ”€â”€ tests/              # Test suite (pytest)
â”œâ”€â”€ examples/           # Usage examples
â”œâ”€â”€ benchmarks/         # Performance benchmarks
â””â”€â”€ docs/               # Documentation
```

---

## ğŸ› ï¸ Development

```bash
# Setup
uv pip install -e ".[dev]"
pre-commit install

# Run checks
pytest                    # Tests
pytest --cov=ragnarok_ai  # With coverage
ruff check . --fix        # Lint
ruff format .             # Format  
mypy src/                 # Type check
```

See [CLAUDE.md](CLAUDE.md) for detailed development guidelines.

---

## ğŸ¤ Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Areas where help is appreciated:**
- Additional metrics implementations
- Framework adapters (Haystack, LlamaIndex)
- Documentation & examples
- Performance optimizations

---

## ğŸ“„ License

This project is licensed under the [AGPL-3.0 License](LICENSE).

**Why AGPL?** To ensure improvements stay open-source and prevent the "open-core bait-and-switch" model where companies take OSS code and monetize without contributing back.

---

## ğŸ™ Acknowledgments

Inspired by the frustrations with existing RAG evaluation tools. Built to be what [Giskard](https://github.com/Giskard-AI/giskard) should have been â€” fast, local, and developer-friendly.

---

<p align="center">
  <sub>Built with â¤ï¸ in Lyon, France</sub>
</p>
